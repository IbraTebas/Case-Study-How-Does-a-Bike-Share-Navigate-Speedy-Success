sql <- "Select MAX(started_at) as latest_no_station, MIN(started_at) as earliest_no_station
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where end_lat is not NULL)
WHERE start_station_name is NULL"
tb <- bq_project_query(projectid, sql)
projectid = 'tidal-tower-348213'
sql <- "Select MAX(started_at) as latest_no_station, MIN(started_at) as earliest_no_station
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where end_lat is not NULL)
WHERE start_station_name is NULL"
tb <- bq_project_query(projectid, sql)
bigrquery::bq_auth(path="CAPSTONE_KEYS.json")
projectid = 'tidal-tower-348213'
sql <- "Select MAX(started_at) as latest_no_station, MIN(started_at) as earliest_no_station
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where end_lat is not NULL)
WHERE start_station_name is NULL"
tb <- bq_project_query(projectid, sql)
bigrquery::bq_auth(email="ibrahim.volpi.vaz@gmail.com")
projectid = 'tidal-tower-348213'
sql <- "Select MAX(started_at) as latest_no_station, MIN(started_at) as earliest_no_station
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where end_lat is not NULL)
WHERE start_station_name is NULL"
tb <- bq_project_query(projectid, sql)
library(bigrquery)
bigrquery::bq_auth(email="ibrahim.volpi.vaz@gmail.com")
projectid = 'tidal-tower-348213'
sql <- "Select MAX(started_at) as latest_no_station, MIN(started_at) as earliest_no_station
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where end_lat is not NULL)
WHERE start_station_name is NULL"
tb <- bq_project_query(projectid, sql)
bigrquery::bq_auth(path = 'CAPSTONE_KEYS.json' )
projectid = 'tidal-tower-348213'
sql <- "Select MAX(started_at) as latest_no_station, MIN(started_at) as earliest_no_station
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where end_lat is not NULL)
WHERE start_station_name is NULL"
tb <- bq_project_query(projectid, sql)
View(tb)
View(tb)
View(tb)
View(tb)
sql <- "select column, countif(value = 'null') as nulls_count
from `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` t,
unnest(array(
select as struct trim(arr[offset(0)], '"') column, trim(arr[offset(1)], '"') value
from unnest(split(trim(to_json_string(t), '{}'))) kv,
unnest([struct(split(kv, ':') as arr)])
)) rec
group by column "
tb <- bq_project_query(projectid, sql)
projectid = 'tidal-tower-348213'
sql <- "SELECT COUNT(*) as member_type_trips, member_casual
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where  end_lat is not NULL)
group by
member_casual"
tb <- bq_project_query(projectid, sql)
View(tb)
View(tb)
tb.show()
tb.str
?tb
??tb
tb
tb_df
tb_df()
library(dbplyr)
tb
tbl(projectid, sql)
library(dplyr)
tb
View(tb)
View(tb)
projectid = 'tidal-tower-348213'
sql <- "SELECT COUNT(*) as member_type_trips, member_casual
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where  end_lat is not NULL)
group by
member_casual"
tbl(projectid, sql)
projectid = 'tidal-tower-348213'
sql <- "SELECT COUNT(*) as member_type_trips, member_casual
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where  end_lat is not NULL)
group by
member_casual"
tb <- bq_project_query(projectid, sql)
View(tb)
View(tb)
table <-bq_table()
table <-bq_table(projectid)
table <-bq_table(projectid, 'tidal-tower-348213.Data_trips_12_month.Data_trips_12_months')
table <-bq_table("data")
samples <- bq_dataset("publicdata", "samples")
natality <- bq_table("publicdata", "samples", "natality")
tb <- bq_project_query(projectid, sql, natality )
samples <- bq_dataset(projectid, "samples")
natality <- bq_table(projectid, "samples", "natality")
tb <- bq_project_query(projectid, sql, natality )
samples <- bq_dataset(projectid, "Data_trips_12_months")
table <- bq_table(projectid, "samples")
table <- bq_table(projectid, "samples", name)
natality <- bq_table(projectid, "samples", "natality")
View(natality)
View(natality)
tb <- bq_project_query(projectid, sql, natality )
samples <- bq_dataset(projectid, ride_id)
samples <- bq_dataset(projectid, 'ride_id')
started_at <- bq_table(projectid, "ride_id", "started_at")
tb <- bq_project_query(projectid, sql, started_at )
projectid = 'tidal-tower-348213'
sql <- "SELECT COUNT(*) as member_type_trips, member_casual
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where  end_lat is not NULL)
group by
member_casual"
tb <- bq_project_query(projectid, sql)
bq_table_download(tb, max_results = 1000)
bq_table_download(tb)
table <-- bq_table_download(tb)
data <--bq_table_download(tb).as.data.frame()
if (bq_testable()) {
df <- bq_table_download(tb, n_max = 35000)
}
df
tb <- as.data.frame(bq_project_query(projectid, sql))
data <--as.data.frame(bq_table_download(tb))
tb <- bq_project_query(projectid, sql)
data <--as.data.frame(bq_table_download(tb))
data <--as.data.frame(bq_table_download(tb, n_max=Inf))
as.data.frame(bq_table_download(tb, n_max=Inf))
data(as.data.frame(bq_table_download(tb, n_max=Inf)))
as.data.frame(bq_table_download(tb, n_max=Inf))
as.data.frame(bq_table_download(tb, n_max=Inf)) --> tester
Tester <- as.data.frame(bq_table_download(tb, n_max=Inf))
View(Tester)
View(Tester)
View(Tester)
View(Tester)
View(Tester)
View(Tester)
View(Tester)
View(Tester)
View(Tester)
View(Tester)
ggplot(data = Tester) +
geom_bar(mapping = aes(x = member_casual, y = member_type_trips))
library(ggplot2)
library(tidyverse)
ggplot(data = Tester) +
geom_bar(mapping = aes(x = member_casual, y = member_type_trips))
ggplot(data = Tester) +
geom_bar(mapping = aes(x = member_casual)
ggplot(data = Tester) +
geom_bar(mapping = aes(x = member_casual))
ggplot(data = Tester) +
geom_bar(mapping = aes(x = member_casual))
View(Tester)
View(Tester)
ggplot(data = Tester) +
geom_bar(aes(x = member_casual, y=member_type_trips))
ggplot(data = Tester) +
geom_bar(mapping = aes(x = member_casual, y=member_type_trips))
View(Tester)
View(Tester)
ggplot(Tester, aes(x = member_casual, y=member_type_trips)) + geom_col()
ggplot(Tester, aes(x = member_casual, y=member_type_trips)) + geom_col() + scale_y_continuous(breaks= pretty_breaks())
library(gridExtra)
install.packages('gridExtra')
library(gridExtra)
ggplot(Tester, aes(x = member_casual, y=member_type_trips)) + geom_col() + scale_y_continuous(breaks= pretty_breaks())
install.packages('scales')
install.packages("scales")
library(scales)
ggplot(Tester, aes(x = member_casual, y=member_type_trips)) + geom_col() + scale_y_continuous(breaks= pretty_breaks())
ggplot(Tester, aes(x = member_casual, y=member_type_trips)) + geom_col() + scale_y_continuous(breaks= pretty_breaks())
ggplot(Tester, aes(x = member_casual, y=member_type_trips)) + geom_col() + scale_y_continuous(breaks= c(1000000,2000000,3000000))
marks =c(1000000,2000000,3000000)
axis(2,at=marks,labels=marks)
ggplot(Tester, aes(x = member_casual, y=member_type_trips)) + geom_col() + axis(2,at=marks,labels=marks) )
ggplot(Tester, aes(x = member_casual, y=member_type_trips)) + geom_col())
axis(2,at=marks,labels=marks)
ggplot(Tester)
axis(2,at=marks,labels=marks)
ggplot(Tester, aes(x = member_casual, y=member_type_trips)) + geom_col()+ scale_x_continuous(labels = comma))
ggplot(Tester, aes(x = member_casual, y=member_type_trips)) + geom_col()+ scale_x_continuous(labels = comma)
ggplot(Tester, aes(x = member_casual, y=member_type_trips)) + geom_col()+ scale_y_continuous(labels = comma)
projectid = 'tidal-tower-348213'
sql_trips_number_type <- "SELECT COUNT(*) as member_type_trips, member_casual
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where  end_lat is not NULL)
group by
member_casual"
sql <- "SELECT COUNT(*) as member_type_trips, member_casual
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where  end_lat is not NULL)
group by
member_casual"
tb_trips_numbers_type <- bq_project_query(projectid, sql_trips_number_type)
Number_of_trips_type <- as.data.frame(bq_table_download(tb_trips_numbers_type, n_max=Inf))  #in n_max it can be restricted the amount of results recovered. Convenient for results with less than 100 MB
ggplot(Number_of_trips_type, aes(x = member_casual, y=member_type_trips)) + geom_col()+ scale_y_continuous(labels = comma)
View(Number_of_trips_type)
View(Number_of_trips_type)
sql_rideable_and_user_type <- "SELECT COUNT(*) as member_type_trips, member_casual, rideable_type
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where  end_lat is not NULL)
group by
member_casual, rideable_type"
tb_trips_rideable_and_user_type <- bq_project_query(projectid, sql_rideable_and_user_type)
Number_of_rideable_and_user_type <- as.data.frame(bq_table_download(tb_trips_rideable_and_user_type, n_max=Inf))
View(Number_of_rideable_and_user_type)
sql_rideable_and_user_type <- "SELECT COUNT(*) as member_type_trips, member_casual, rideable_type
FROM (SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months` where  end_lat is not NULL)
group by
member_casual, rideable_type
ORDER BY
member_casual"
tb_trips_rideable_and_user_type <- bq_project_query(projectid, sql_rideable_and_user_type)
Number_of_rideable_and_user_type <- as.data.frame(bq_table_download(tb_trips_rideable_and_user_type, n_max=Inf))
View(Number_of_rideable_and_user_type)
View(Number_of_rideable_and_user_type)
### EXPORT to R
install.packages('bigrquery')
install.packages('gridExtra')
install.packages('scales')
library(bigrquery)
library(gridExtra)
library(scales)
bigrquery::bq_auth(path = 'CAPSTONE_KEYS.json' )
sql_all_trips_12_months <- "SELECT * FROM `tidal-tower-348213.Data_trips_12_month.Data_trips_12_months`"
tb_all_trips_12_months <- bq_project_query(projectid, sql_all_trips_12_months)
data_12_months <- as.data.frame(bq_table_download(tb_all_trips_12_months, n_max=Inf))
View(data_12_months)
View(data_12_months)
write.csv(data_12_months,"data_12_months.csv", row.names = TRUE)
View(data_12_months)
View(data_12_months)
write.csv(data_12_months,".\data_12_months.csv", row.names = TRUE)
write.csv(data_12_months,"D:\data_12_months.csv", row.names = TRUE)
write.csv(data_12_months,"D:/data_12_months.csv", row.names = TRUE)
View(Number_of_rideable_and_user_type)
View(Number_of_rideable_and_user_type)
haversine<- function(long1, lat1, long2, lat2) {
stopifnot(is.numeric(long1),
is.numeric(lat1),
is.numeric(long2),
is.numeric(lat2),
long1 > -180,
long1 < 180,
lat1 > -180,
lat1 < 180,
long2 > -180,
long2 < 180,
lat2 > -180,
lat2 < 180
)
long1 <- long1*pi/180
lat1 <- lat1*pi/180
long2 <- long2*pi/180
lat2 <- lat2*pi/180
R <- 6371 # Earth mean radius [km]
delta.long <- (long2 - long1)
delta.lat <- (lat2 - lat1)
a <- sin(delta.lat/2)^2 + cos(lat1) * cos(lat2) * sin(delta.long/2)^2
c <- 2 * asin(min(1,sqrt(a)))
d = R * c
return(d) # Distance in km
}
data <- read.csv("data_12_months.csv")
data <- data %>%
distinct(ride_id, .keep_all = TRUE) %>%                        #eliminate duplicate rows
drop_na(end_lat)                                               #eliminate rows with NaN in the end_lat column less thank 5k detected with NAN count line in Appendix
data$start_time<- format(as.POSIXct(                           #extract date and time in separate columns at start and end
data$started_at),format = "%H:%M:%S")
data$start_date <- as.Date (data$started_at)
data$end_time<- format(as.POSIXct(
data$ended_at),format = "%H:%M:%S")
data$end_date <- as.Date (data$ended_at)
#add the day of the week
data$weekday_started = format(data$start_date, format = "%a")                                #*as a string abbreviated
data$weekday_started_int = as.numeric(format(data$start_date, format = "%u"))                #* as an integer, Monday = 1
data$weekday_ended = format(data$end_date, format = "%a")
data$weekday_ended_int = as.numeric(format(data$end_date, format = "%u"))                    #idem
#calculate duration
data$duration_trip_in_mins <- difftime(data$ended_at, data$started_at, units = "mins") #let's check some stats
mean(data$duration_trip_in_mins)
max(data$duration_trip_in_mins)
min(data$duration_trip_in_mins)
#removed the date columns with combination of day and time
data <- subset(data, select = -c(started_at, ended_at))
#add the column distance_in_km to the data
data$distance_in_km <- with(data, mapply(haversine, lat1=data$start_lat, long1=data$start_lng, lat2=data$end_lat, long2=data$end_lng))
#Shows the MAX of any numerical column, it is detected the outlier potentially observed on Tableau
data %>% summarise_if(is.numeric, max)
data %>%summarise_if(is.numeric, min)
#let's detect outliers
outliers_distance_max <- data %>% filter(distance_in_km > 1000)
outliers_distance_and_duration <- data %>% filter(distance_in_km == 0 & duration_trip_in_mins < 5)# we will remove the trips that the bike is docked in the same station and less than 5 min have passed. Candidate to abandon trip.
Outlier_graph <-data %>% filter(ride_id == "9F438AD0AB380E3F" )  #ride id detected as an outlier on Tableau when checking end lat and lng.
outlier_time_more_than_5_hrs <- data %>% filter(duration_trip_in_mins > 300)  #check trip duration greater than 5 hours
outlier_time_null_time_or_negative <- data %>% filter(duration_trip_in_mins < 0 ) # check negative durations
#Removal of the correspondent rows for the outliers
data <- subset(data, distance_in_km != 0 | duration_trip_in_mins > 4)     #removal of 131855 rows
data <- subset(data, distance_in_km < 1000)  #distance more than 1000KM, tested max and it went down to 114.3836
data <- subset(data, duration_trip_in_mins <= 300 & duration_trip_in_mins >=0) #removal of  9419 rows with more of 5 hours duration, 1161 of them more than a day.
data <- subset(data, ride_id != "9F438AD0AB380E3F")
#Remove NaN
data_no_NaN <- na.omit(data)
#Convert trip duration to float.
data_no_NaN$duration_trip_in_mins <- as.numeric(data_no_NaN$duration_trip_in_mins)
data <- read.csv("data_12_months.csv")
data <- data %>%
distinct(ride_id, .keep_all = TRUE) %>%                        #eliminate duplicate rows
drop_na(end_lat)
data <- data %>%
distinct(ride_id, .keep_all = TRUE)
View(data)
View(data)
write.csv(data_no_NaN,"D:/data_12_months_clean.csv", row.names = TRUE)
library(dplyr)
data <- read.csv("data_12_months_clean.csv")
View(data)
View(data)
data <- read.csv("data_12_months_clean.csv")
data <- read.csv("data_12_months_clean.csv")
data <- read.csv("data_12_months_clean.csv")
View(data)
View(data)
library(dplyr)
library(leaflet)
library(tidyverse)
library(geosphere)
library(matrixStats)
data <- read.csv("data_12_months.csv")
data <- data %>%
distinct(ride_id, .keep_all = TRUE) %>%                       #eliminate duplicate rows
drop_na(end_lat)                                              #eliminate rows with NaN in the end_lat column less thank 5k detected with NAN count line in Appendix
#extract date and time in separate columns at start and end
data$start_time<- format(as.POSIXct(
data$started_at),format = "%H:%M:%S")
data$start_date <- as.Date (data$started_at)
data$end_time<- format(as.POSIXct(
data$ended_at),format = "%H:%M:%S")
data$end_date <- as.Date (data$ended_at)
#add the day of the week
data$weekday_started = format(data$start_date, format = "%a")                                #*as a string abbreviated
data$weekday_started_int = as.numeric(format(data$start_date, format = "%u"))                #* as an integer, Monday = 1
data$weekday_ended = format(data$end_date, format = "%a")
data$weekday_ended_int = as.numeric(format(data$end_date, format = "%u"))                    #idem
#calculate duration
data$duration_trip_in_mins <- difftime(data$ended_at, data$started_at, units = "mins") #let's check some stats
mean(data$duration_trip_in_mins)
max(data$duration_trip_in_mins)
min(data$duration_trip_in_mins)
#removed the date columns with combination of day and time
data <- subset(data, select = -c(started_at, ended_at))
#add the column distance_in_km to the data
#Function for distance.
haversine<- function(long1, lat1, long2, lat2) {
stopifnot(is.numeric(long1),
is.numeric(lat1),
is.numeric(long2),
is.numeric(lat2),
long1 > -180,
long1 < 180,
lat1 > -180,
lat1 < 180,
long2 > -180,
long2 < 180,
lat2 > -180,
lat2 < 180
)
long1 <- long1*pi/180
lat1 <- lat1*pi/180
long2 <- long2*pi/180
lat2 <- lat2*pi/180
R <- 6371 # Earth mean radius [km]
delta.long <- (long2 - long1)
delta.lat <- (lat2 - lat1)
a <- sin(delta.lat/2)^2 + cos(lat1) * cos(lat2) * sin(delta.long/2)^2
c <- 2 * asin(min(1,sqrt(a)))
d = R * c
return(d) # Distance in km
}
#add the column distance_in_km to the data
data$distance_in_km <- with(data, mapply(haversine, lat1=data$start_lat, long1=data$start_lng, lat2=data$end_lat, long2=data$end_lng))
#Shows the MAX of any numerical column, it is detected the outlier potentially observed on Tableau
data %>% summarise_if(is.numeric, max)
data %>%summarise_if(is.numeric, min)
#Removal of the correspondent rows for the outliers
data <- subset(data, distance_in_km != 0 | duration_trip_in_mins > 4)     #removal of 131855 rows
data <- subset(data, distance_in_km < 1000)  #distance more than 1000KM, tested max and it went down to 114.3836
data <- subset(data, duration_trip_in_mins <= 300 & duration_trip_in_mins >=0) #removal of  9419 rows with more of 5 hours duration, 1161 of them more than a day.
data <- subset(data, ride_id != "9F438AD0AB380E3F")
#Remove NaN
data_no_NaN <- na.omit(data)
write.csv(data_no_NaN,"D:\\data_12_months_clean.csv", row.names = TRUE)
data_clean <- read.csv("data_12_months_clean.csv")
data_clean <- read.csv("data_12_months_clean.csv")
data_clean <- read.csv("data_12_months_clean.csv")
View(data_clean)
View(data_no_NaN)
View(data_clean)
View(data_clean)
View(data_clean)
View(data_no_NaN)
write.csv(data_no_NaN,"D:\\data_clean.csv", row.names = TRUE)
write.csv(data_no_NaN,"D:\\data_clean_final.csv", row.names = FALSE)
library(dplyr)
#Load data cleaned
data_clean <- read.csv("data_clean_final.csv", header = TRUE)
library(dplyr)
data_clean <- read.csv("data_clean_final.csv", header = TRUE)
library(dplyr)
data_clean <- read.csv("data_clean_final.csv", header = TRUE)
data_clean <- read.csv("data_clean_final.csv")
#Load data cleaned
data_clean <- read.csv("data_clean_final.csv")
data_clean <- read.csv("data_clean_final.csv", header = TRUE)
library(dplyr)
#Load data cleaned
data_clean <- read.csv("data_clean_final.csv", header = TRUE)
data_clean <- read.csv("data_clean_final.csv")
data_clean <- read.csv("data_clean_final.csv")
library(dplyr)
library(tidyverse)
data_cleaned <- read.csv("data_clean_final.csv")
library(tidyverse)
data_cleaned <- read.csv('data_clean_final.csv')
install.packages("leaflet")
install.packages('geosphere')
install.packages('matrixStats')
install.packages('dplyr')
install.packages('plyr')
install.packages("dplyr")
install.packages('vtable')
library(dplyr)
library(leaflet)
library(tidyverse)
library(geosphere)
library(matrixStats)
data <- read.csv("data_12_months.csv")
data <- data %>%
distinct(ride_id, .keep_all = TRUE) %>%
drop_na(end_lat)
#extract date and time in separate columns at start and end
data$start_time<- format(as.POSIXct(
data$started_at),format = "%H:%M:%S")
data$start_date <- as.Date (data$started_at)
data$end_time<- format(as.POSIXct(
data$ended_at),format = "%H:%M:%S")
data$end_date <- as.Date (data$ended_at)
#add the day of the week
data$weekday_started = format(data$start_date, format = "%a")                                #*as a string abbreviated
data$weekday_started_int = as.numeric(format(data$start_date, format = "%u"))                #* as an integer, Monday = 1
data$weekday_ended = format(data$end_date, format = "%a")
data$weekday_ended_int = as.numeric(format(data$end_date, format = "%u"))                    #idem
#calculate duration
data$duration_trip_in_mins <- difftime(data$ended_at, data$started_at, units = "mins") #let's check some stats
mean(data$duration_trip_in_mins)
max(data$duration_trip_in_mins)
min(data$duration_trip_in_mins)
#removed the date columns with combination of day and time
data <- subset(data, select = -c(started_at, ended_at))
haversine<- function(long1, lat1, long2, lat2) {
stopifnot(is.numeric(long1),
is.numeric(lat1),
is.numeric(long2),
is.numeric(lat2),
long1 > -180,
long1 < 180,
lat1 > -180,
lat1 < 180,
long2 > -180,
long2 < 180,
lat2 > -180,
lat2 < 180
)
long1 <- long1*pi/180
lat1 <- lat1*pi/180
long2 <- long2*pi/180
lat2 <- lat2*pi/180
R <- 6371 # Earth mean radius [km]
delta.long <- (long2 - long1)
delta.lat <- (lat2 - lat1)
a <- sin(delta.lat/2)^2 + cos(lat1) * cos(lat2) * sin(delta.long/2)^2
c <- 2 * asin(min(1,sqrt(a)))
d = R * c
return(d) # Distance in km
}
data$distance_in_km <- with(data, mapply(haversine, lat1=data$start_lat, long1=data$start_lng, lat2=data$end_lat, long2=data$end_lng))
data %>% summarise_if(is.numeric, max)
data %>%summarise_if(is.numeric, min)
outliers_distance_max <- data %>% filter(distance_in_km > 1000)
outliers_distance_and_duration <- data %>% filter(distance_in_km == 0 & duration_trip_in_mins < 5)# we will remove the trips that the bike is docked in the same station and less than 5 min have passed. Candidate to abandon trip.
Outlier_graph <-data %>% filter(ride_id == "9F438AD0AB380E3F" )  #ride id detected as an outlier on Tableau when checking end lat and lng.
outlier_time_more_than_5_hrs <- data %>% filter(duration_trip_in_mins > 300)  #check trip duration greater than 5 hours
outlier_time_null_time_or_negative <- data %>% filter(duration_trip_in_mins < 0 ) # check negative duration
#Removal of the correspondent rows for the outliers
data <- subset(data, distance_in_km != 0 | duration_trip_in_mins > 4)     #removal of 131855 rows
data <- subset(data, distance_in_km < 1000)  #distance more than 1000KM, tested max and it went down to 114.3836
data <- subset(data, duration_trip_in_mins <= 300 & duration_trip_in_mins >=0) #removal of  9419 rows with more of 5 hours duration, 1161 of them more than a day.
data <- subset(data, ride_id != "9F438AD0AB380E3F")
install.packages('reticulate')
load.library(reticulate)
install.packages('RcppTOM')
install.packages('reticulate')
load.library(reticulate)
install.packages('RcppTOM')
install.packages('reticulate')
library(reticulate)
library('RcppTOM')
install.packages('Rtools')
install.packages('reticulate')
library(reticulate)
library('Rtools')
install.packages("reticulate")
install.packages('reticulate')
library('reticulate')
install.packages("reticulate")
setwd("D:/DataScience/Google Data Science course/CAPSTONE/Data _cleaning/DATA_cleaning_R")
install.packages('pdflatex')
